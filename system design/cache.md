# what is caching ?
Caching is a technique used to temporarily store copies of data in high-speed storage layers (such as RAM) to reduce the time taken to access data.

The primary goal of caching is to improve system performance by reducing latency, offloading the main data store, and providing faster data retrieval.

---
# Why Use Caching?
Caching is essential for the following reasons:

**Improved Performance**: By storing frequently accessed data in a cache, the time required to retrieve that data is significantly reduced.

**Reduced Load on Backend Systems**: Caching reduces the number of requests that need to be processed by the backend, freeing up resources for other operations.

**Increased Scalability**: Caches help in handling a large number of read requests, making the system more scalable.

**Cost Efficiency**: By reducing the load on backend systems, caching can help lower infrastructure costs.

**Enhanced User Experience**: Faster response times lead to a better user experience, particularly for web and mobile applications
# 2. Types of caching
## 2.1 In-Memory Cache
In-memory caches store data in the main memory (RAM) for extremely fast access.

These caches are typically used for session management, storing frequently accessed objects, and as a front for databases.

Examples: Redis and Memcached.

## 2.2 Distributed Cache
A distributed cache spans multiple servers and is designed to handle large-scale systems.

It ensures that cached data is available across different nodes in a distributed system.

Examples: Redis Cluster and Amazon ElastiCache.

## 2.3 Client-Side Cache
Client-side caching involves storing data on the client device, typically in the form of cookies, local storage, or application-specific caches.

This is commonly used in web browsers to cache static assets like images, scripts, and stylesheets.

## 2.4 Database Cache
Database caching involves storing frequently queried database results in a cache.

This reduces the number of queries made to the database, improving performance and scalability.

## 2.5 Content Delivery Network (CDN)
A CDN is a geographically distributed network of servers that work together to deliver web content (like HTML pages, JavaScript files, stylesheets, images, and videos) to users based on their geographic location.

The primary purpose of a CDN is to deliver content to end-users with high availability and performance by reducing the physical distance between the server and the user.

When a user requests content from a website, the CDN redirects the request to the nearest server in its network, reducing latency and improving load times.

### 2.5.2. How Does a CDN Work?
A CDN operates using three key components:

**Edge Servers** ‚Äì Located at Points of Presence (PoP) locations, these servers cache and deliver content closer to users.

**Origin Servers** ‚Äì The primary servers where the original content is stored.

**DNS (Domain Name System)** ‚Äì Directs user requests to the nearest edge server instead of the origin server.

---
CDNs use a Time-to-Live (TTL) mechanism to determine how long content remains cached before expiring. To ensure users always receive the latest version, CDNs periodically refresh and update cached content from the origin server.

## 2.5.3. Benefits of Using a CDN
**Faster Load Times** ‚Äì By serving content from the nearest edge server, CDNs reduce latency and improve page load speed.

**Reduced Server Load** ‚Äì CDNs offload traffic from the origin server by caching static assets, reducing resource consumption.

**Improved Availability and Reliability** ‚Äì With multiple servers in different locations, CDNs prevent single points of failure.

**Scalability**: CDNs can handle traffic spikes more efficiently than traditional hosting, making them ideal for websites with fluctuating traffic patterns.

**Global Reach**: CDNs make it easier to deliver content to users worldwide, regardless of their location.

**Enhanced Security** ‚Äì Many CDNs offer DDoS protection, Web Application Firewalls (WAFs), and bot mitigation to secure applications.

While CDNs offer many benefits, it‚Äôs important to note that they also introduce some challenges like:

**Increased Complexity**: Integrating a CDN requires proper DNS configuration, cache rules, and content invalidation policies.

**Increased Cost**: Many CDN providers charge based on bandwidth usage and request volume. For high-traffic websites, CDN costs can be substantial, especially for video streaming, gaming, and software distribution.

# üîÑ Pull CDN vs Push CDN

## üì• Pull CDN (Origin Pull)

### üîπ How It Works
- CDN fetches content **on demand** from the origin when first requested.
- It then caches that content for subsequent requests.

### üîÅ Request Flow

### ‚úÖ Pros
- Easy to set up
- Automatically keeps up with updates from origin
- Good for frequently updated content

### ‚ùå Cons
- First request may be slow (cache miss)
- Relies on origin being available

### ‚úÖ Best For
- CMS sites
- Dynamic websites
- API caching

---

## üì§ Push CDN (Content Push)

### üîπ How It Works
- You **manually upload** static files to the CDN.
- CDN **distributes** content to edge nodes in advance.

### üîÅ Request Flow

### ‚úÖ Pros
- Always fast response
- No dependency on origin server
- Ideal for large static assets

### ‚ùå Cons
- Requires manual management
- Not suitable for frequently changing data

### ‚úÖ Best For
- Static sites
- Video streaming
- Software distribution


---
# 3. Caching Strategies
## 3.1 Read Through
In the Read Through strategy, the cache acts as an intermediary between the application and the database.

When the application requests data, it first looks in the cache.

If data is available (cache hit), it‚Äôs returned to the application.

If the data is not available (cache miss), the cache itself is responsible for fetching the data from the database, storing it, and returning it to the application.

To prevent the cache from serving stale data, a time-to-live (TTL) can be added to cached entries. TTL automatically expires the data after a specified duration, allowing it to be reloaded from the database when needed.

Read Through caching is best suited for read-heavy applications where data is accessed frequently but updated less often, such as content delivery systems (CDNs), social media feeds, or user profiles.

## 3.2. Cache Aside

Cache Aside, also known as "Lazy Loading", is a strategy where the application code handles the interaction between the cache and the database. The data is loaded into the cache only when needed.

The application first checks the cache for data. If the data exists in cache (cache hit), it‚Äôs returned to the application.

If the data isn't found in cache (cache miss), the application retrieves it from the database (or the primary data store), then loads it into the cache for subsequent requests.

To avoid stale data, we can set a time-to-live (TTL) for cached data. Once the TTL expires, the data is automatically removed from the cache.

Cache Aside is perfect for systems where the read-to-write ratio is high, and data updates are infrequent. For example, in an e-commerce website, product data (like prices, descriptions, or stock status) is often read much more frequently than it's updated.

## 3.3 Write Through
In the Write Through strategy, every write operation is executed on both the cache and the database at the same time.

This is a synchronous process, meaning both the cache and the database are updated as part of the same operation, ensuring that there is no delay in data propagation.

This approach ensures that the cache and the database remain synchronized and the read requests from the cache will always return the latest data, avoiding the risk of serving stale data.

In a Write Through caching strategy, cache expiration policies (such as TTL) are generally not necessary. However, if you are concerned about cache memory usage, you can implement a TTL policy to remove infrequently accessed data after a certain time period.

The biggest advantage of Write Through is that it ensures strong data consistency between the cache and the database.

Since the cache always contains the latest data, read operations benefit from low latency because data can be directly retrieved from the cache.

However, write latency can be higher due to the overhead of writing to both the cache and the database.

Write Through is ideal for consistency-critical systems, such as financial applications or online transaction processing systems, where the cache and database must always have the latest data.

## 3.4. Write Around
Write Around is a caching strategy where data is written directly to the database, bypassing the cache.

The cache is only updated when the data is requested later during a read operation, at which point the Cache Aside strategy is used to load the data into the cache.

This approach ensures that only frequently accessed data resides in the cache, preventing it from being polluted by data that may not be accessed again soon.

It keeps the cache clean by avoiding unnecessary data that might not be requested after being written.

Writes are relatively faster because they only target the database and don‚Äôt incur the overhead of writing to the cache.

TTL can be used to ensure that data does not remain in the cache indefinitely. Once the TTL expires, the data is removed from the cache, forcing the system to retrieve it from the database again if needed.

Write Around caching is best used in write-heavy systems where data is frequently written or updated, but not immediately or frequently read such as logging systems.

## 3.5. Write Back
In the Write Back strategy, data is first written to the cache and then asynchronously written to the database at a later time.

This strategy focuses on minimizing write latency by deferring database writes.

This deferred writing means that the cache acts as the primary storage during write operations, while the database is updated periodically in the background.

The key advantage of Write Back is that it significantly reduces write latency, as writes are completed quickly in the cache, and the database updates are delayed or batched.

However, with this approach, there is a risk of data loss if the cache fails before the data has been written to the database.

This can be mitigated by using persistent caching solutions like Redis with AOF (Append Only File), which logs every write operation to disk, ensuring data durability even if the cache crashes.

Write Back doesn't require invalidation of cache entries, as the cache itself is the source of truth during the write process.

Write Back caching is ideal for write-heavy scenarios where write operations need to be fast and frequent, but immediate consistency with the database is not critical, such as logging systems and social media feeds.

# 4. Cache Eviction Policies
## 4.1. Least Recently Used (LRU)
LRU evicts the item that hasn‚Äôt been used for the longest time.

The idea is simple: if you haven‚Äôt accessed an item in a while, it‚Äôs less likely to be accessed again soon.

### Prose
**Intuitive**: Easy to understand and widely adopted.

**Efficient**: Keeps frequently accessed items in the cache.

**Optimized for Real-World Usage**: Matches many access patterns, such as web browsing and API calls.

## Cons:
**Metadata Overhead**: Tracking usage order can consume additional memory.

**Performance Cost**: For large caches, maintaining the access order may introduce computational overhead.

**Not Adaptive**: Assumes past access patterns will predict future usage, which may not always hold true.

## 4.2. Least Frequently Used (LFU)
LFU evicts the item with the lowest access frequency. It assumes that items accessed less frequently in the past are less likely to be accessed in the future.

Unlike LRU, which focuses on recency, LFU emphasizes frequency of access.

## Pros:
Efficient for Predictable Patterns: Retains frequently accessed data, which is often more relevant.

Highly Effective for Popular Data: Works well in scenarios with clear "hot" items.

## Cons:
High Overhead: Requires additional memory to track frequency counts.

Slower Updates: Tracking and updating frequency can slow down operations.

Not Adaptive: May keep items that were frequently accessed in the past but are no longer relevant.

# 4.3 First In, First Out (FIFO)
FIFO evicts the item that was added first, regardless of how often it‚Äôs accessed.

FIFO operates under the assumption that items added earliest are least likely to be needed as the cache fills up.

## Pros:
Simple to Implement: FIFO is straightforward and requires minimal logic.

Low Overhead: No need to track additional metadata like access frequency or recency.

Deterministic Behavior: Eviction follows a predictable order.

Cons:
Ignores Access Patterns: Items still in frequent use can be evicted, reducing cache efficiency.

Suboptimal for Many Use Cases: FIFO is rarely ideal in modern systems where recency and frequency matter.

May Waste Cache Space: If old but frequently used items are evicted, the cache loses its utility.

## 4.4. Random Replacement (RR)
RR cache eviction strategy is the simplest of all: when the cache is full, it evicts a random item to make space for a new one.

It doesn't track recency, frequency, or insertion order, making it a lightweight approach with minimal computational overhead.

Pros:
Simple to Implement: No need for metadata like access frequency or recency.

Low Overhead: Computational and memory requirements are minimal.

Fair for Unpredictable Access Patterns: Avoids bias toward recency or frequency, which can be useful in some scenarios.

Cons:
Unpredictable Eviction: A frequently used item might be evicted, reducing cache efficiency.

Inefficient for Stable Access Patterns: Doesn‚Äôt adapt well when certain items are consistently accessed.

High Risk of Poor Cache Hit Rates: Random eviction often leads to suboptimal retention of important items.

## 4.5. Most Recently Used (MRU)
MRU is the opposite of Least Recently Used (LRU). In MRU, the item that was accessed most recently is the first to be evicted when the cache is full.

The idea behind MRU is that the most recently accessed item is likely to be a temporary need and won‚Äôt be accessed again soon, so evicting it frees up space for potentially more valuable data.

Pros:
Effective in Specific Scenarios: Retains older data, which might be more valuable in certain workloads.

Simple Implementation: Requires minimal metadata.

Cons:
Suboptimal for Most Use Cases: MRU assumes recent data is less valuable, which is often untrue for many applications.

Poor Hit Rate in Predictable Patterns: Fails in scenarios where recently accessed data is more likely to be reused.

Rarely Used in Practice: Limited applicability compared to other strategies like LRU or LFU.

# 4.6. Time to Live (TTL)
TTL is a cache eviction strategy where each cached item is assigned a fixed lifespan. Once an item‚Äôs lifespan expires, it is automatically removed from the cache, regardless of access patterns or frequency.

This ensures that cached data remains fresh and prevents stale data from lingering in the cache indefinitely.

Pros:
Ensures Freshness: Automatically removes stale data, ensuring only fresh items remain in the cache.

Simple to Configure: TTL values are easy to assign during cache insertion.

Low Overhead: No need to track usage patterns or access frequency.

Prevents Memory Leaks: Stale data is cleared out systematically, avoiding cache bloat.

Cons:
Fixed Lifespan: Items may be evicted prematurely even if they are frequently accessed.

Wasteful Eviction: Items that haven‚Äôt expired but are still irrelevant occupy cache space.

Limited Flexibility: TTL doesn‚Äôt adapt to dynamic workloads or usage patterns.
## 4.7. Two-Tiered Caching
Two-Tiered Caching combines two layers of cache‚Äîusually a local cache (in-memory) and a remote cache (distributed or shared).

The local cache serves as the first layer (hot cache), providing ultra-fast access to frequently used data, while the remote cache acts as the second layer (cold cache) for items not found in the local cache but still needed relatively quickly.
Pros:
Ultra-Fast Access: Local cache provides near-instantaneous response times for frequent requests.

Scalable Storage: Remote cache adds scalability and allows data sharing across multiple servers.

Reduces Database Load: Two-tiered caching significantly minimizes calls to the backend database.

Fault Tolerance: If the local cache fails, the remote cache acts as a fallback.

Cons:
Complexity: Managing two caches introduces more overhead, including synchronization and consistency issues.

Stale Data: Inconsistent updates between tiers may lead to serving stale data.

Increased Latency for Remote Cache Hits: Accessing the second-tier remote cache is slower than the local cache.

# 5. Challenges and Considerations

**Cache Coherence**: Ensuring that data in the cache remains consistent with the source of truth (e.g., the database).

**Cache Invalidation**: Determining when and how to update or remove stale data from the cache.

**Cold Start**: Handling scenarios when the cache is empty, such as after a system restart.

**Cache Eviction Policies**: Deciding which items to remove when the cache reaches capacity (e.g., Least Recently Used, Least Frequently Used).

**Cache Penetration**: Preventing malicious attempts to repeatedly query for non-existent data, potentially overwhelming the backend.

**Cache Stampede**: Managing situations where many concurrent requests attempt to rebuild the cache simultaneously.
---
# 6. Best Practices for Implementing Caching
**Cache the Right Data**: Focus on caching data that is expensive to compute or retrieve and that is frequently accessed.

**Set Appropriate TTLs**: Use TTLs to automatically invalidate cache entries and prevent stale data.

**Consider Cache Warming**: Preload essential data into the cache to avoid cold starts.

**Monitor Cache Performance**: Regularly monitor cache hit/miss ratios and adjust caching strategies based on usage patterns.

**Use Layered Caching**: Implement caching at multiple layers (e.g., client-side, server-side, CDN) to maximize performance benefits.

**Handle Cache Misses Gracefully**: Ensure that the system can handle cache misses efficiently without significant performance degradation

---
# 7. Conclusion
Caching is a powerful technique in system design that, when implemented correctly, can drastically improve the performance, scalability, and cost-efficiency of a system.

However, it comes with its own set of challenges, particularly around consistency and invalidation.

By understanding the different types of caches, cache placement strategies, and best practices, you can design a robust caching strategy that meets the needs of your application.
